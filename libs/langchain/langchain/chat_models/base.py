import asyncio
import inspect
import warnings
from abc import ABC, abstractmethod
from functools import partial
from typing import (
    Any,
    AsyncIterator,
    Dict,
    Iterator,
    List,
    Optional,
    Sequence,
    cast,
)

from langchain.callbacks.base import BaseCallbackManager
from langchain.callbacks.manager import (
    AsyncCallbackManager,
    AsyncCallbackManagerForLLMRun,
    CallbackManager,
    CallbackManagerForLLMRun,
    Callbacks,
)
from langchain.globals import get_llm_cache
from langchain.load.dump import dumpd, dumps
from langchain.prompts.base import StringPromptValue
from langchain.prompts.chat import ChatPromptValue
from langchain.pydantic_v1 import Field, root_validator
from langchain.schema import (
    ChatGeneration,
    ChatResult,
    LLMResult,
    PromptValue,
    RunInfo,
)
from langchain.schema.language_model import BaseLanguageModel, LanguageModelInput
from langchain.schema.messages import (
    AIMessage,
    AnyMessage,
    BaseMessage,
    BaseMessageChunk,
    HumanMessage,
)
from langchain.schema.output import ChatGenerationChunk
from langchain.schema.runnable import RunnableConfig


def _get_verbosity() -> bool:
    from langchain.globals import get_verbose

    return get_verbose()


def _generate_from_stream(stream: Iterator[ChatGenerationChunk]) -> ChatResult:
    generation: Optional[ChatGenerationChunk] = None
    for chunk in stream:
        if generation is None:
            generation = chunk
        else:
            generation += chunk
    assert generation is not None
    return ChatResult(generations=[generation])


async def _agenerate_from_stream(
    stream: AsyncIterator[ChatGenerationChunk],
) -> ChatResult:
    generation: Optional[ChatGenerationChunk] = None
    async for chunk in stream:
        if generation is None:
            generation = chunk
        else:
            generation += chunk
    assert generation is not None
    return ChatResult(generations=[generation])


"""
è¿™æ˜¯ä¸€ä¸ªPythonä»£ç ç‰‡æ®µï¼Œç”¨äºå®šä¹‰ä¸€ä¸ªåä¸º`BaseChatModel`çš„æŠ½è±¡ç±»ï¼Œå®ƒæ˜¯ä¸€ä¸ªç”¨äºèŠå¤©æ¨¡å‹çš„åŸºç±»ï¼Œç»§æ‰¿äº†`BaseLanguageModel`ç±»ï¼Œå¹¶æä¾›äº†ä¸€äº›æ–¹æ³•æ¥æ ¹æ®è¾“å…¥çš„æ¶ˆæ¯ç”ŸæˆèŠå¤©å†…å®¹Â¹ã€‚

`BaseLanguageModel[BaseMessage]`æ˜¯ä¸€ä¸ªæ³›å‹ç±»ï¼Œå®ƒè¡¨ç¤ºè¿™ä¸ªç±»å¯ä»¥æ¥å—å’Œè¿”å›`BaseMessage`ç±»å‹çš„å¯¹è±¡ã€‚`BaseMessage`æ˜¯ä¸€ä¸ªç”¨äºè¡¨ç¤ºèŠå¤©æ¶ˆæ¯çš„åŸºç±»ï¼Œå®ƒåŒ…å«äº†æ¶ˆæ¯çš„æ–‡æœ¬ã€å…ƒæ•°æ®ã€æ ‡ç­¾ç­‰å±æ€§Â²ã€‚ä½¿ç”¨æ³›å‹ç±»å¯ä»¥è®©ä»£ç æ›´çµæ´»å’Œå¯é‡ç”¨ï¼Œå› ä¸ºå®ƒå¯ä»¥é€‚åº”ä¸åŒç±»å‹çš„è¾“å…¥å’Œè¾“å‡ºÂ³ã€‚

`ABC`æ˜¯ä¸€ä¸ªç”¨äºåˆ›å»ºæŠ½è±¡ç±»çš„å…ƒç±»ï¼Œå®ƒè¡¨ç¤ºè¿™ä¸ªç±»ä¸èƒ½è¢«å®ä¾‹åŒ–ï¼Œåªèƒ½è¢«å…¶ä»–ç±»ç»§æ‰¿ï¼Œå¹¶ä¸”å¿…é¡»å®ç°ä¸€äº›æŠ½è±¡æ–¹æ³•ã€‚ä½¿ç”¨æŠ½è±¡ç±»å¯ä»¥è®©ä»£ç æ›´è§„èŒƒå’Œæ¸…æ™°ï¼Œå› ä¸ºå®ƒå¯ä»¥å®šä¹‰ä¸€äº›é€šç”¨çš„æ¥å£å’Œé€»è¾‘ï¼Œè®©å­ç±»æ ¹æ®å…·ä½“çš„éœ€æ±‚æ¥å®ç°ã€‚

ä»¥ä¸Šå°±æ˜¯æˆ‘å¯¹è¿™ä¸ªä»£ç ç‰‡æ®µçš„è§£é‡Šã€‚å¸Œæœ›å¯¹ä½ æœ‰å¸®åŠ©ã€‚ğŸ˜Š

æº: ä¸å¿…åº”çš„å¯¹è¯ï¼Œ 2023/10/28
(1) langchain.schema.language_model .BaseLanguageModel. https://api.python.langchain.com/en/latest/schema/langchain.schema.language_model.BaseLanguageModel.html.
(2) BaseLanguageModel<RunOutput, CallOptions> | ï¸ Langchain. https://js.langchain.com/docs/api/base_language/classes/BaseLanguageModel.
(3) BaseLanguageModel class - langchain library - Dart API - Pub. https://pub.dev/documentation/langchain/latest/langchain/BaseLanguageModel-class.html.
"""

class BaseChatModel(BaseLanguageModel[BaseMessage], ABC):
    # çœ‹çœ‹è¿™ä¸ªæ¨¡å‹ç±»éœ€è¦å®ç°å“ªäº›æ–¹æ³•, æˆ‘è¦è‡ªå·±ç»§æ‰¿ä¸€ä¸ª
    """Base class for Chat models."""

    cache: Optional[bool] = None
    """Whether to cache the response."""
    verbose: bool = Field(default_factory=_get_verbosity)
    """Whether to print out response text."""
    callbacks: Callbacks = Field(default=None, exclude=True)
    """Callbacks to add to the run trace."""
    callback_manager: Optional[BaseCallbackManager] = Field(default=None, exclude=True)  # å­—æ®µå·²ç»åºŸå¼ƒäº†
    """Callback manager to add to the run trace."""
    tags: Optional[List[str]] = Field(default=None, exclude=True)
    """Tags to add to the run trace."""
    metadata: Optional[Dict[str, Any]] = Field(default=None, exclude=True)
    """Metadata to add to the run trace."""

    @root_validator()
    def raise_deprecation(cls, values: Dict) -> Dict:
        """Raise deprecation warning if callback_manager is used."""
        if values.get("callback_manager") is not None:
            warnings.warn(
                "callback_manager is deprecated. Please use callbacks instead.",
                DeprecationWarning,
            )
            values["callbacks"] = values.pop("callback_manager", None)
        return values

    class Config:
        """Configuration for this pydantic object."""

        """
        GitHub Copilot: `arbitrary_types_allowed = True` æ˜¯ Pydantic æ¨¡å‹çš„ä¸€ä¸ªé…ç½®é€‰é¡¹ï¼Œç”¨äºå…è®¸æ¨¡å‹ä¸­åŒ…å«ä»»æ„ç±»å‹çš„å±æ€§ã€‚
        é»˜è®¤æƒ…å†µä¸‹ï¼ŒPydantic åªå…è®¸ä½¿ç”¨é¢„å®šä¹‰çš„æ•°æ®ç±»å‹ï¼Œä¾‹å¦‚ intã€strã€float ç­‰ã€‚
        å¦‚æœéœ€è¦åœ¨æ¨¡å‹ä¸­ä½¿ç”¨è‡ªå®šä¹‰ç±»å‹ï¼Œéœ€è¦å°† `arbitrary_types_allowed` è®¾ç½®ä¸º Trueã€‚
        è¿™ä¸ªé€‰é¡¹é€šå¸¸åœ¨æ¨¡å‹ä¸­å®šä¹‰äº†è‡ªå®šä¹‰ç±»å‹çš„æ—¶å€™ä½¿ç”¨ã€‚
        """
        arbitrary_types_allowed = True

    # --- Runnable methods ---

    @property
    def OutputType(self) -> Any:
        # å®šä¹‰è¾“å‡ºç±»å‹
        """Get the output type for this runnable."""
        return AnyMessage

    def _convert_input(self, input: LanguageModelInput) -> PromptValue:
        """
        è½¬æ¢è¾“å…¥
        """
        if isinstance(input, PromptValue):
            return input
        elif isinstance(input, str):
            return StringPromptValue(text=input)
        elif isinstance(input, list):
            return ChatPromptValue(messages=input)
        else:
            raise ValueError(
                f"Invalid input type {type(input)}. "
                "Must be a PromptValue, str, or list of BaseMessages."
            )

    def invoke(
        self,
        input: LanguageModelInput,
        config: Optional[RunnableConfig] = None,
        *,
        stop: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> BaseMessage:
        """
        è°ƒç”¨
        """
        config = config or {}
        return cast(
            ChatGeneration,  # è½¬æ¢æˆè¿™ä¸ªç±»å‹, ä»…ä»…æ˜¯è¡¨ç¤ºç†æƒ³çš„è¾“å‡ºåº”è¯¥æ˜¯è¿™ä¸ªç±»å‹, å®é™…ä¸Š cast å‡½æ•°ä»€ä¹ˆéƒ½ä¸åš
            # è°ƒç”¨ç”Ÿæˆæ–¹æ³•
            self.generate_prompt(
                # å…ˆè½¬æ¢è¾“å…¥
                [self._convert_input(input)],
                stop=stop,
                # éƒ½æ˜¯ä» config ä¸­è·å–çš„
                callbacks=config.get("callbacks"),
                tags=config.get("tags"),
                metadata=config.get("metadata"),
                run_name=config.get("run_name"),
                **kwargs,
            ).generations[0][0],
        ).message

    async def ainvoke(
        self,
        input: LanguageModelInput,
        config: Optional[RunnableConfig] = None,
        *,
        stop: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> BaseMessage:
        """
        å¼‚æ­¥çš„, TODO: è¿˜æ²¡çœ‹
        """
        config = config or {}
        llm_result = await self.agenerate_prompt(
            [self._convert_input(input)],
            stop=stop,
            callbacks=config.get("callbacks"),
            tags=config.get("tags"),
            metadata=config.get("metadata"),
            run_name=config.get("run_name"),
            **kwargs,
        )
        return cast(ChatGeneration, llm_result.generations[0][0]).message

    def stream(
        self,
        input: LanguageModelInput,
        config: Optional[RunnableConfig] = None,
        *,
        stop: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> Iterator[BaseMessageChunk]:
        """
        æµå¼çš„, TODO: è¿˜æ²¡çœ‹
        """
        if type(self)._stream == BaseChatModel._stream:
            # model doesn't implement streaming, so use default implementation
            yield cast(
                BaseMessageChunk, self.invoke(input, config=config, stop=stop, **kwargs)
            )
        else:
            config = config or {}
            messages = self._convert_input(input).to_messages()
            params = self._get_invocation_params(stop=stop, **kwargs)
            options = {"stop": stop, **kwargs}
            callback_manager = CallbackManager.configure(
                config.get("callbacks"),
                self.callbacks,
                self.verbose,
                config.get("tags"),
                self.tags,
                config.get("metadata"),
                self.metadata,
            )
            (run_manager,) = callback_manager.on_chat_model_start(
                dumpd(self),
                [messages],
                invocation_params=params,
                options=options,
                name=config.get("run_name"),
            )
            try:
                generation: Optional[ChatGenerationChunk] = None
                for chunk in self._stream(
                    messages, stop=stop, run_manager=run_manager, **kwargs
                ):
                    yield chunk.message
                    if generation is None:
                        generation = chunk
                    else:
                        generation += chunk
                assert generation is not None
            except BaseException as e:
                run_manager.on_llm_error(e)
                raise e
            else:
                run_manager.on_llm_end(
                    LLMResult(generations=[[generation]]),
                )

    async def astream(
        self,
        input: LanguageModelInput,
        config: Optional[RunnableConfig] = None,
        *,
        stop: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> AsyncIterator[BaseMessageChunk]:
        if type(self)._astream == BaseChatModel._astream:
            # model doesn't implement streaming, so use default implementation
            yield cast(
                BaseMessageChunk, self.invoke(input, config=config, stop=stop, **kwargs)
            )
        else:
            config = config or {}
            messages = self._convert_input(input).to_messages()
            params = self._get_invocation_params(stop=stop, **kwargs)
            options = {"stop": stop, **kwargs}
            callback_manager = AsyncCallbackManager.configure(
                config.get("callbacks"),
                self.callbacks,
                self.verbose,
                config.get("tags"),
                self.tags,
                config.get("metadata"),
                self.metadata,
            )
            (run_manager,) = await callback_manager.on_chat_model_start(
                dumpd(self),
                [messages],
                invocation_params=params,
                options=options,
                name=config.get("run_name"),
            )
            try:
                generation: Optional[ChatGenerationChunk] = None
                async for chunk in self._astream(
                    messages, stop=stop, run_manager=run_manager, **kwargs
                ):
                    yield chunk.message
                    if generation is None:
                        generation = chunk
                    else:
                        generation += chunk
                assert generation is not None
            except BaseException as e:
                await run_manager.on_llm_error(e)
                raise e
            else:
                await run_manager.on_llm_end(
                    LLMResult(generations=[[generation]]),
                )

    # --- Custom methods ---

    def _combine_llm_outputs(self, llm_outputs: List[Optional[dict]]) -> dict:
        return {}

    def _get_invocation_params(
        self,
        stop: Optional[List[str]] = None,
        **kwargs: Any,
    ) -> dict:
        # åŠ ä¸Šè‡ªèº«çš„å‚æ•°, åˆå¹¶èµ·æ¥
        params = self.dict()
        params["stop"] = stop
        return {**params, **kwargs}

    def _get_llm_string(self, stop: Optional[List[str]] = None, **kwargs: Any) -> str:
        if self.is_lc_serializable():
            params = {**kwargs, **{"stop": stop}}
            param_string = str(sorted([(k, v) for k, v in params.items()]))
            llm_string = dumps(self)
            return llm_string + "---" + param_string
        else:
            params = self._get_invocation_params(stop=stop, **kwargs)
            params = {**params, **kwargs}
            return str(sorted([(k, v) for k, v in params.items()]))

    def generate(
        self,
        messages: List[List[BaseMessage]],
        stop: Optional[List[str]] = None,
        callbacks: Callbacks = None,
        *,
        tags: Optional[List[str]] = None,
        metadata: Optional[Dict[str, Any]] = None,
        run_name: Optional[str] = None,
        **kwargs: Any,
    ) -> LLMResult:
        # çœ‹æ¥è¿™ä¸ªæ‰æ˜¯æ ¸å¿ƒæ–¹æ³•
        """Top Level call"""
        params = self._get_invocation_params(stop=stop, **kwargs)
        options = {"stop": stop}

        callback_manager = CallbackManager.configure(
            callbacks,
            self.callbacks,
            self.verbose,
            tags,
            self.tags,
            metadata,
            self.metadata,
        )
        run_managers = callback_manager.on_chat_model_start(
            dumpd(self),
            messages,
            invocation_params=params,
            options=options,
            name=run_name,
        )
        results = []
        # å¤„ç†æ¯ä¸ªæ¶ˆæ¯
        for i, m in enumerate(messages):
            try:
                results.append(
                    # è°ƒç”¨ç”Ÿæˆæ–¹æ³•
                    self._generate_with_cache(
                        m,
                        stop=stop,
                        run_manager=run_managers[i] if run_managers else None,
                        **kwargs,
                    )
                )
            except BaseException as e:
                if run_managers:
                    run_managers[i].on_llm_error(e)
                raise e
        # å¯¹æ¯ä¸ªè¾“å‡ºè°ƒç”¨ LLMResult
        flattened_outputs = [
            LLMResult(generations=[res.generations], llm_output=res.llm_output)
            for res in results
        ]
        # ç»„åˆ llm_output
        llm_output = self._combine_llm_outputs([res.llm_output for res in results])
        # æ‰€æœ‰çš„ generations
        generations = [res.generations for res in results]
        # å†ç»„åˆä¸‹, è¿™å°±æ˜¯æœ€åçš„è¾“å‡º
        output = LLMResult(generations=generations, llm_output=llm_output)
        if run_managers:
            run_infos = []
            for manager, flattened_output in zip(run_managers, flattened_outputs):
                manager.on_llm_end(flattened_output)
                run_infos.append(RunInfo(run_id=manager.run_id))
            output.run = run_infos
        return output

    async def agenerate(
        self,
        messages: List[List[BaseMessage]],
        stop: Optional[List[str]] = None,
        callbacks: Callbacks = None,
        *,
        tags: Optional[List[str]] = None,
        metadata: Optional[Dict[str, Any]] = None,
        run_name: Optional[str] = None,
        **kwargs: Any,
    ) -> LLMResult:
        """Top Level call"""
        params = self._get_invocation_params(stop=stop, **kwargs)
        options = {"stop": stop}

        callback_manager = AsyncCallbackManager.configure(
            callbacks,
            self.callbacks,
            self.verbose,
            tags,
            self.tags,
            metadata,
            self.metadata,
        )

        run_managers = await callback_manager.on_chat_model_start(
            dumpd(self),
            messages,
            invocation_params=params,
            options=options,
            name=run_name,
        )

        results = await asyncio.gather(
            *[
                self._agenerate_with_cache(
                    m,
                    stop=stop,
                    run_manager=run_managers[i] if run_managers else None,
                    **kwargs,
                )
                for i, m in enumerate(messages)
            ],
            return_exceptions=True,
        )
        exceptions = []
        for i, res in enumerate(results):
            if isinstance(res, BaseException):
                if run_managers:
                    await run_managers[i].on_llm_error(res)
                exceptions.append(res)
        if exceptions:
            if run_managers:
                await asyncio.gather(
                    *[
                        run_manager.on_llm_end(
                            LLMResult(
                                generations=[res.generations], llm_output=res.llm_output
                            )
                        )
                        for run_manager, res in zip(run_managers, results)
                        if not isinstance(res, Exception)
                    ]
                )
            raise exceptions[0]
        flattened_outputs = [
            LLMResult(generations=[res.generations], llm_output=res.llm_output)
            for res in results
        ]
        llm_output = self._combine_llm_outputs([res.llm_output for res in results])
        generations = [res.generations for res in results]
        output = LLMResult(generations=generations, llm_output=llm_output)
        await asyncio.gather(
            *[
                run_manager.on_llm_end(flattened_output)
                for run_manager, flattened_output in zip(
                    run_managers, flattened_outputs
                )
            ]
        )
        if run_managers:
            output.run = [
                RunInfo(run_id=run_manager.run_id) for run_manager in run_managers
            ]
        return output

    def generate_prompt(
        self,
        prompts: List[PromptValue],
        stop: Optional[List[str]] = None,
        callbacks: Callbacks = None,
        **kwargs: Any,
    ) -> LLMResult:
        # å°†æ¨¡æ¿è½¬æ¢æˆæ¶ˆæ¯
        prompt_messages = [p.to_messages() for p in prompts]
        # è°ƒç”¨ç”Ÿæˆæ–¹æ³•
        return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)

    async def agenerate_prompt(
        self,
        prompts: List[PromptValue],
        stop: Optional[List[str]] = None,
        callbacks: Callbacks = None,
        **kwargs: Any,
    ) -> LLMResult:
        prompt_messages = [p.to_messages() for p in prompts]
        return await self.agenerate(
            prompt_messages, stop=stop, callbacks=callbacks, **kwargs
        )

    def _generate_with_cache(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        # çœ‹ä¸‹è¿™ä¸ª, æ˜¯ä¸ªåŒ…è£… cache åŠŸèƒ½çš„
        new_arg_supported = inspect.signature(self._generate).parameters.get(
            "run_manager"
        )
        # å¿½ç•¥ç¼“å­˜. ä»…å½“ cache ä¸æ˜¯ None ä¸” cache ä¸º False æ—¶
        disregard_cache = self.cache is not None and not self.cache
        llm_cache = get_llm_cache()
        if llm_cache is None or disregard_cache:
            # llm.cache æ˜¯å…·ä½“å®ç°, self.cache æ˜¯å¸ƒå°”å€¼, ç”¨æ¥é…ç½®æ˜¯å¦ç¼“å­˜
            # This happens when langchain.cache is None, but self.cache is True
            if self.cache is not None and self.cache:
                raise ValueError(
                    "Asked to cache, but no cache found at `langchain.cache`."
                )
            if new_arg_supported:
                # è°ƒç”¨ç”Ÿæˆæ–¹æ³•
                return self._generate(
                    messages, stop=stop, run_manager=run_manager, **kwargs
                )
            else:
                return self._generate(messages, stop=stop, **kwargs)
        else:
            # æœ‰ç¼“å­˜çš„æ—¶å€™
            llm_string = self._get_llm_string(stop=stop, **kwargs)
            prompt = dumps(messages)
            # è°ƒç”¨ llm_cache.lookup çš„æ–¹æ³•, ä»ç¼“å­˜ä¸­è·å–
            cache_val = llm_cache.lookup(prompt, llm_string)
            if isinstance(cache_val, list):
                return ChatResult(generations=cache_val)
            else:
                # å¿…é¡»è¦æ±‚ cache_val ä¸º list, å¦åˆ™è¿˜æ˜¯ä¼šè°ƒç”¨ _generate æ–¹æ³•
                if new_arg_supported:
                    result = self._generate(
                        messages, stop=stop, run_manager=run_manager, **kwargs
                    )
                else:
                    result = self._generate(messages, stop=stop, **kwargs)
                # æ›´æ–°ç¼“å­˜
                llm_cache.update(prompt, llm_string, result.generations)
                return result

    async def _agenerate_with_cache(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        new_arg_supported = inspect.signature(self._agenerate).parameters.get(
            "run_manager"
        )
        disregard_cache = self.cache is not None and not self.cache
        llm_cache = get_llm_cache()
        if llm_cache is None or disregard_cache:
            # This happens when langchain.cache is None, but self.cache is True
            if self.cache is not None and self.cache:
                raise ValueError(
                    "Asked to cache, but no cache found at `langchain.cache`."
                )
            if new_arg_supported:
                return await self._agenerate(
                    messages, stop=stop, run_manager=run_manager, **kwargs
                )
            else:
                return await self._agenerate(messages, stop=stop, **kwargs)
        else:
            llm_string = self._get_llm_string(stop=stop, **kwargs)
            prompt = dumps(messages)
            cache_val = llm_cache.lookup(prompt, llm_string)
            if isinstance(cache_val, list):
                return ChatResult(generations=cache_val)
            else:
                if new_arg_supported:
                    result = await self._agenerate(
                        messages, stop=stop, run_manager=run_manager, **kwargs
                    )
                else:
                    result = await self._agenerate(messages, stop=stop, **kwargs)
                llm_cache.update(prompt, llm_string, result.generations)
                return result

    @abstractmethod
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        # è¿™ä¸ªæ˜¯è¦å­ç±»å®ç°çš„
        """Top Level call"""

    async def _agenerate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """Top Level call"""
        return await asyncio.get_running_loop().run_in_executor(
            None, partial(self._generate, **kwargs), messages, stop, run_manager
        )

    def _stream(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Iterator[ChatGenerationChunk]:
        raise NotImplementedError()

    def _astream(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> AsyncIterator[ChatGenerationChunk]:
        raise NotImplementedError()

    def __call__(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        callbacks: Callbacks = None,
        **kwargs: Any,
    ) -> BaseMessage:
        generation = self.generate(
            [messages], stop=stop, callbacks=callbacks, **kwargs
        ).generations[0][0]
        if isinstance(generation, ChatGeneration):
            return generation.message
        else:
            raise ValueError("Unexpected generation type")

    async def _call_async(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        callbacks: Callbacks = None,
        **kwargs: Any,
    ) -> BaseMessage:
        result = await self.agenerate(
            [messages], stop=stop, callbacks=callbacks, **kwargs
        )
        generation = result.generations[0][0]
        if isinstance(generation, ChatGeneration):
            return generation.message
        else:
            raise ValueError("Unexpected generation type")

    def call_as_llm(
        self, message: str, stop: Optional[List[str]] = None, **kwargs: Any
    ) -> str:
        return self.predict(message, stop=stop, **kwargs)

    def predict(
        self, text: str, *, stop: Optional[Sequence[str]] = None, **kwargs: Any
    ) -> str:
        if stop is None:
            _stop = None
        else:
            _stop = list(stop)
        result = self([HumanMessage(content=text)], stop=_stop, **kwargs)
        return result.content

    def predict_messages(
        self,
        messages: List[BaseMessage],
        *,
        stop: Optional[Sequence[str]] = None,
        **kwargs: Any,
    ) -> BaseMessage:
        if stop is None:
            _stop = None
        else:
            _stop = list(stop)
        return self(messages, stop=_stop, **kwargs)

    async def apredict(
        self, text: str, *, stop: Optional[Sequence[str]] = None, **kwargs: Any
    ) -> str:
        if stop is None:
            _stop = None
        else:
            _stop = list(stop)
        result = await self._call_async(
            [HumanMessage(content=text)], stop=_stop, **kwargs
        )
        return result.content

    async def apredict_messages(
        self,
        messages: List[BaseMessage],
        *,
        stop: Optional[Sequence[str]] = None,
        **kwargs: Any,
    ) -> BaseMessage:
        if stop is None:
            _stop = None
        else:
            _stop = list(stop)
        return await self._call_async(messages, stop=_stop, **kwargs)

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """Get the identifying parameters."""
        return {}

    @property
    @abstractmethod
    def _llm_type(self) -> str:
        """Return type of chat model."""

    def dict(self, **kwargs: Any) -> Dict:
        """Return a dictionary of the LLM."""
        starter_dict = dict(self._identifying_params)
        starter_dict["_type"] = self._llm_type
        return starter_dict


class SimpleChatModel(BaseChatModel):
    """Simple Chat Model."""

    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        # è°ƒç”¨ _call æ–¹æ³•, è¿”å›å­—ç¬¦ä¸²
        output_str = self._call(messages, stop=stop, run_manager=run_manager, **kwargs)
        # å°è£…ä¸‹æ¨¡å‹è¿”å›çš„å­—ç¬¦ä¸²
        message = AIMessage(content=output_str)
        # å†å°è£…ä¸‹
        generation = ChatGeneration(message=message)
        # æœ€åç»„æˆ ChatResult
        return ChatResult(generations=[generation])

    @abstractmethod
    def _call(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        # éœ€è¦å­ç±»å®ç°
        """Simpler interface."""

    async def _agenerate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        # ä¹Ÿå°±æ˜¯è¯´å‚æ•°éƒ½å·²ç»é¢„å…ˆä¼ å…¥äº†, æ‰€ä»¥å¼‚æ­¥è°ƒç”¨çš„æ—¶å€™ä¸éœ€è¦ä¼ å…¥
        func = partial(
            self._generate, messages, stop=stop, run_manager=run_manager, **kwargs
        )
        # å…·ä½“æ‰§è¡Œ, å¼‚æ­¥æ‰§è¡Œ
        return await asyncio.get_event_loop().run_in_executor(None, func)
